{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to change accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\dnl01\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\statsmodels\\compat\\pandas.py:61: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import Int64Index as NumericIndex\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from Sarima import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_method(df,column_name,methode,direction,order,limit):\n",
    "  df[column_name] = df[column_name].interpolate(method=methode,limit_direction = direction,order=order,limit=limit)\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_csv(df,name,time_variable,quantitative_variable):\n",
    "  df = df[[time_variable, quantitative_variable]].copy()\n",
    "  df.columns = ['Date','y']\n",
    "  df['y'] = (df['y'].apply(pd.to_numeric, errors='coerce')).to_frame()\n",
    "  df = df.drop_duplicates(subset=['Date'])\n",
    "  df['Date'] = pd.to_datetime(df.Date, errors = 'coerce')\n",
    "  df = df.dropna(subset=['Date'])\n",
    "  df = df.set_index('Date').to_period(name[0])\n",
    "  df = df.sort_values(by=['Date'])\n",
    "  df = df.dropna()\n",
    "  pourcentage = ((len(df) - df['y'].count()) / len(df)) * 100\n",
    "  if((pourcentage > 2) and (pourcentage < 25) and (2/3 * len(df)) > 400):\n",
    "    df = interpolate_method(df,'y',\"polynomial\",None,3,None)\n",
    "    df=(df-df.min())/(df.max()-df.min())\n",
    "  elif((pourcentage >= 25) or (2/3 * len(df)) < 600):\n",
    "    df=(df-df.min())/(df.max()-df.min())\n",
    "    #print(\"La base de donnée comporte un grand nombre de données manquantes pour être étudiée, ou n'est pas assez grande.\")\n",
    "    return df, False\n",
    "  df=(df-df.min())/(df.max()-df.min())\n",
    "  return df, True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: M-data_Afghanistan.csv ()\n",
      "Processed: M-data_Albania.csv ()\n",
      "Processed: M-data_Algeria.csv ()\n",
      "Processed: M-data_Andorra.csv ()\n",
      "Processed: M-data_Antigua_Barbada.csv ()\n",
      "Processed: M-data_Armenia.csv ()\n",
      "Processed: M-data_Azerbaijan.csv ()\n",
      "Processed: M-data_Bahamas.csv ()\n",
      "Processed: M-data_Baker_Island.csv ()\n",
      "Processed: M-data_Belgium.csv ()\n",
      "Processed: M-data_Benin.csv ()\n",
      "Processed: M-data_Bolivia.csv ()\n",
      "Processed: M-data_Bosnia_H.csv ()\n",
      "Processed: M-data_Botswana.csv ()\n",
      "Processed: M-data_British_Virgin_Island.csv ()\n",
      "Processed: M-data_Burma.csv ()\n",
      "Processed: M-data_Cameroon.csv ()\n",
      "Processed: M-data_Colombia.csv ()\n",
      "Processed: M-data_Costa_Rica.csv ()\n",
      "END 19 19\n"
     ]
    }
   ],
   "source": [
    "results_300 = pd.DataFrame(columns=['DATA_SET_NAME','Method','MAE','MSE','RMSE'], dtype=object)\n",
    "results_200 = pd.DataFrame(columns=['DATA_SET_NAME','Method','MAE','MSE','RMSE'], dtype=object)\n",
    "results_100 = pd.DataFrame(columns=['DATA_SET_NAME','Method','MAE','MSE','RMSE'], dtype=object)\n",
    "\n",
    "fil = glob.glob(\"*.csv\")\n",
    "i = 0\n",
    "\n",
    "for name in fil:\n",
    "    d = pd.read_csv(name, parse_dates=True, sep=\",\")\n",
    "    df = d.copy()\n",
    "    df, b = clean_csv(df,name,'Date','Value')\n",
    "    df['y'] = pd.to_numeric(df['y'])\n",
    "    n = len(df)\n",
    "    df_300t,df_200t,df_100t = df[:-300]['y'].to_frame(),df[:-200]['y'].to_frame(),df[:-100]['y'].to_frame()\n",
    "    df_300v,df_200v,df_100v = df[n-300:]['y'].to_frame(),df[n-200:]['y'].to_frame(),df[n-100:]['y'].to_frame()\n",
    "\n",
    "    try:\n",
    "        s_300,s_200,s_100 = Sarima(df_300t),Sarima(df_200t),Sarima(df_100t)\n",
    "        #pred_300, pred_ci_300 = s_300.predict(start=pd.to_datetime(str((df.iloc[n-300:,:]).iloc[0,].name).split(\" \")[0]), end=pd.to_datetime(str((df.iloc[n-300:,:]).iloc[-1,].name).split(\" \")[0]), dynamic=False, alpha=0.1)\n",
    "        #pred_200, pred_ci_200 = s_200.predict(start=pd.to_datetime(str((df.iloc[n-200:,:]).iloc[0,].name).split(\" \")[0]), end=pd.to_datetime(str((df.iloc[n-200:,:]).iloc[-1,].name).split(\" \")[0]), dynamic=False, alpha=0.1)\n",
    "        #pred_100, pred_ci_100 = s_100.predict(start=pd.to_datetime(str((df.iloc[n-100:,:]).iloc[0,].name).split(\" \")[0]), end=pd.to_datetime(str((df.iloc[n-300:,:]).iloc[-1,].name).split(\" \")[0]), dynamic=False, alpha=0.1)\n",
    "        pred_300, pred_ci_300 = s_300.predict(start=df_300v.index[0], end=df_300v.index[-1], dynamic=False, alpha=0.1)\n",
    "        pred_200, pred_ci_200 = s_200.predict(start=df_200v.index[0], end=df_200v.index[-1], dynamic=False, alpha=0.1)\n",
    "        pred_100, pred_ci_100 = s_100.predict(start=df_100v.index[0], end=df_100v.index[-1], dynamic=False, alpha=0.1)\n",
    "        df_new_row_300 = pd.DataFrame(data=np.array([[name,'Sarima',float(s_300.mae(data=df_300v)),float(s_300.mse(data=df_300v)),float(s_300.rmse(data=df_300v))]]), columns=['DATA_SET_NAME','Method','MAE','MSE','RMSE'])\n",
    "        df_new_row_200 = pd.DataFrame(data=np.array([[name,'Sarima',float(s_200.mae(data=df_200v)),float(s_200.mse(data=df_200v)),float(s_200.rmse(data=df_200v))]]), columns=['DATA_SET_NAME','Method','MAE','MSE','RMSE'])\n",
    "        df_new_row_100 = pd.DataFrame(data=np.array([[name,'Sarima',float(s_100.mae(data=df_100v)),float(s_100.mse(data=df_100v)),float(s_100.rmse(data=df_100v))]]), columns=['DATA_SET_NAME','Method','MAE','MSE','RMSE'])\n",
    "        results_300 = pd.concat([results_300,df_new_row_300], ignore_index=True)\n",
    "        results_200 = pd.concat([results_200,df_new_row_200], ignore_index=True)\n",
    "        results_100 = pd.concat([results_100,df_new_row_100], ignore_index=True)\n",
    "        print(f\"Processed: {name} {np.array(s_300.prediction).shape}\")\n",
    "        i+=1\n",
    "    except Exception as e:\n",
    "        #print(s.season)\n",
    "        sr = str(e)\n",
    "        print(f\"Error: {name}\\n{sr}\")\n",
    "        print(f\"300: pred={np.array(s_300.prediction.predicted_mean).shape}, {df_300v.shape}\\n200: pred={np.array(s_200.prediction.predicted_mean).shape}, {df_200v.shape}\\n100: pred={np.array(s_100.prediction.predicted_mean).shape}, {df_100v.shape}\")\n",
    "#print(results)\n",
    "\n",
    "print(f\"END {i} {len(fil)}\")\n",
    "results_300.to_csv(r'Sarima_results_300.csv', index = False)\n",
    "results_200.to_csv(r'Sarima_results_200.csv', index = False)\n",
    "results_100.to_csv(r'Sarima_results_100.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

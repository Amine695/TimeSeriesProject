{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to change accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from Sarima import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "\n",
    "from darts import TimeSeries\n",
    "from darts.dataprocessing.transformers import Scaler\n",
    "from darts.metrics import mae\n",
    "from darts.metrics import mse\n",
    "from darts.metrics import rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_method(df,column_name,methode,direction,order,limit):\n",
    "  df[column_name] = df[column_name].interpolate(method=methode,limit_direction = direction,order=order,limit=limit)\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_csv(df,name,time_variable,quantitative_variable):\n",
    "  df = df[[time_variable, quantitative_variable]].copy()\n",
    "  df.columns = ['Date','y']\n",
    "  df['y'] = (df['y'].apply(pd.to_numeric, errors='coerce')).to_frame()\n",
    "  df = df.drop_duplicates(subset=['Date'])\n",
    "  df['Date'] = pd.to_datetime(df.Date, errors = 'coerce')\n",
    "  df = df.dropna(subset=['Date'])\n",
    "  df = df.set_index('Date').to_period(name[0])\n",
    "  df = df.sort_values(by=['Date'])\n",
    "  df = df.dropna()\n",
    "  pourcentage = ((len(df) - df['y'].count()) / len(df)) * 100\n",
    "  if((pourcentage > 2) and (pourcentage < 25) and (2/3 * len(df)) > 400):\n",
    "    df = interpolate_method(df,'y',\"polynomial\",None,3,None)\n",
    "  elif((pourcentage >= 25) or (2/3 * len(df)) < 600):\n",
    "    #print(\"La base de donnée comporte un grand nombre de données manquantes pour être étudiée, ou n'est pas assez grande.\")\n",
    "    return df, False\n",
    "  if(len(df) > 1000):\n",
    "    supr = len(df) - 1000\n",
    "    df = df[:-supr]\n",
    "  return df, True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: M-data_Bahamas.csv\n",
      "Cannot interpret 'period[M]' as a data type\n",
      "300: pred=(305,), (300, 2)\n",
      "200: pred=(204,), (200, 2)\n",
      "100: pred=(102,), (100, 2)\n",
      "Error: M-data_British_Virgin_Island.csv\n",
      "Cannot interpret 'period[M]' as a data type\n",
      "300: pred=(306,), (300, 2)\n",
      "200: pred=(204,), (200, 2)\n",
      "100: pred=(103,), (100, 2)\n",
      "Error: M-data_Bolivia.csv\n",
      "Cannot interpret 'period[M]' as a data type\n",
      "300: pred=(307,), (300, 2)\n",
      "200: pred=(204,), (200, 2)\n",
      "100: pred=(102,), (100, 2)\n",
      "Error: M-data_Albania.csv\n",
      "Cannot interpret 'period[M]' as a data type\n",
      "300: pred=(304,), (300, 2)\n",
      "200: pred=(202,), (200, 2)\n",
      "100: pred=(101,), (100, 2)\n",
      "Error: M-data_Baker_Island.csv\n",
      "Cannot interpret 'period[M]' as a data type\n",
      "300: pred=(304,), (300, 2)\n",
      "200: pred=(203,), (200, 2)\n",
      "100: pred=(102,), (100, 2)\n",
      "Error: M-data_Algeria.csv\n",
      "Cannot interpret 'period[M]' as a data type\n",
      "300: pred=(305,), (300, 2)\n",
      "200: pred=(202,), (200, 2)\n",
      "100: pred=(102,), (100, 2)\n",
      "Error: M-data_Andorra.csv\n",
      "Cannot interpret 'period[M]' as a data type\n",
      "300: pred=(309,), (300, 2)\n",
      "200: pred=(206,), (200, 2)\n",
      "100: pred=(100,), (100, 2)\n",
      "Error: M-data_Azerbaijan.csv\n",
      "Cannot interpret 'period[M]' as a data type\n",
      "300: pred=(304,), (300, 2)\n",
      "200: pred=(201,), (200, 2)\n",
      "100: pred=(100,), (100, 2)\n",
      "Error: M-data_Belgium.csv\n",
      "Cannot interpret 'period[M]' as a data type\n",
      "300: pred=(306,), (300, 2)\n",
      "200: pred=(202,), (200, 2)\n",
      "100: pred=(100,), (100, 2)\n",
      "Error: M-data_Burma.csv\n",
      "Cannot interpret 'period[M]' as a data type\n",
      "300: pred=(311,), (300, 2)\n",
      "200: pred=(210,), (200, 2)\n",
      "100: pred=(105,), (100, 2)\n",
      "Error: M-data_Afghanistan.csv\n",
      "Cannot interpret 'period[M]' as a data type\n",
      "300: pred=(300,), (300, 2)\n",
      "200: pred=(200,), (200, 2)\n",
      "100: pred=(100,), (100, 2)\n",
      "Error: M-data_Cameroon.csv\n",
      "Cannot interpret 'period[M]' as a data type\n",
      "300: pred=(303,), (300, 2)\n",
      "200: pred=(203,), (200, 2)\n",
      "100: pred=(101,), (100, 2)\n",
      "Error: M-data_Armenia.csv\n",
      "Cannot interpret 'period[M]' as a data type\n",
      "300: pred=(305,), (300, 2)\n",
      "200: pred=(203,), (200, 2)\n",
      "100: pred=(100,), (100, 2)\n",
      "Error: M-data_Costa_Rica.csv\n",
      "Cannot interpret 'period[M]' as a data type\n",
      "300: pred=(304,), (300, 2)\n",
      "200: pred=(203,), (200, 2)\n",
      "100: pred=(101,), (100, 2)\n",
      "Error: M-data_Benin.csv\n",
      "Cannot interpret 'period[M]' as a data type\n",
      "300: pred=(303,), (300, 2)\n",
      "200: pred=(203,), (200, 2)\n",
      "100: pred=(100,), (100, 2)\n",
      "Error: M-data_Bosnia_H.csv\n",
      "Cannot interpret 'period[M]' as a data type\n",
      "300: pred=(305,), (300, 2)\n",
      "200: pred=(203,), (200, 2)\n",
      "100: pred=(100,), (100, 2)\n",
      "Error: M-data_Botswana.csv\n",
      "Cannot interpret 'period[M]' as a data type\n",
      "300: pred=(307,), (300, 2)\n",
      "200: pred=(205,), (200, 2)\n",
      "100: pred=(103,), (100, 2)\n",
      "Error: M-data_Antigua_Barbada.csv\n",
      "Cannot interpret 'period[M]' as a data type\n",
      "300: pred=(309,), (300, 2)\n",
      "200: pred=(205,), (200, 2)\n",
      "100: pred=(103,), (100, 2)\n",
      "Error: M-data_Colombia.csv\n",
      "Cannot interpret 'period[M]' as a data type\n",
      "300: pred=(305,), (300, 2)\n",
      "200: pred=(205,), (200, 2)\n",
      "100: pred=(103,), (100, 2)\n",
      "END 0 19\n"
     ]
    }
   ],
   "source": [
    "results_300 = pd.DataFrame(columns=['DATA_SET_NAME','Method','MAE','MSE','RMSE'], dtype=object)\n",
    "results_200 = pd.DataFrame(columns=['DATA_SET_NAME','Method','MAE','MSE','RMSE'], dtype=object)\n",
    "results_100 = pd.DataFrame(columns=['DATA_SET_NAME','Method','MAE','MSE','RMSE'], dtype=object)\n",
    "\n",
    "os.chdir('../../Datasets/Climat')\n",
    "\n",
    "fil = glob.glob(\"*.csv\")\n",
    "i = 0\n",
    "\n",
    "for name in fil:\n",
    "    d = pd.read_csv(name, parse_dates=True, sep=\",\")\n",
    "    df = d.copy()\n",
    "    df, b = clean_csv(df,name,'Date','Value')\n",
    "    df['y'] = pd.to_numeric(df['y'])\n",
    "    n = len(df)\n",
    "    df_300t,df_200t,df_100t = df[:-300]['y'].to_frame(),df[:-200]['y'].to_frame(),df[:-100]['y'].to_frame()\n",
    "    df_300v,df_200v,df_100v = df[n-300:]['y'].to_frame(),df[n-200:]['y'].to_frame(),df[n-100:]['y'].to_frame()\n",
    "    \n",
    "    try:\n",
    "        s_300,s_200,s_100 = Sarima(df_300t),Sarima(df_200t),Sarima(df_100t)\n",
    "        #pred_300, pred_ci_300 = s_300.predict(start=pd.to_datetime(str((df.iloc[n-300:,:]).iloc[0,].name).split(\" \")[0]), end=pd.to_datetime(str((df.iloc[n-300:,:]).iloc[-1,].name).split(\" \")[0]), dynamic=False, alpha=0.1)\n",
    "        #pred_200, pred_ci_200 = s_200.predict(start=pd.to_datetime(str((df.iloc[n-200:,:]).iloc[0,].name).split(\" \")[0]), end=pd.to_datetime(str((df.iloc[n-200:,:]).iloc[-1,].name).split(\" \")[0]), dynamic=False, alpha=0.1)\n",
    "        #pred_100, pred_ci_100 = s_100.predict(start=pd.to_datetime(str((df.iloc[n-100:,:]).iloc[0,].name).split(\" \")[0]), end=pd.to_datetime(str((df.iloc[n-300:,:]).iloc[-1,].name).split(\" \")[0]), dynamic=False, alpha=0.1)\n",
    "        pred_300, pred_ci_300 = s_300.predict(start=df_300v.index[0], end=df_300v.index[-1], dynamic=False, alpha=0.1)\n",
    "        pred_200, pred_ci_200 = s_200.predict(start=df_200v.index[0], end=df_200v.index[-1], dynamic=False, alpha=0.1)\n",
    "        pred_100, pred_ci_100 = s_100.predict(start=df_100v.index[0], end=df_100v.index[-1], dynamic=False, alpha=0.1)\n",
    "        \n",
    "        df_300t.reset_index(inplace=True)\n",
    "        df_300t = df_300t.rename(columns = {'index':'Date'})\n",
    "        \n",
    "        df_200t.reset_index(inplace=True)\n",
    "        df_200t = df_200t.rename(columns = {'index':'Date'})\n",
    "        \n",
    "        df_100t.reset_index(inplace=True)\n",
    "        df_100t = df_100t.rename(columns = {'index':'Date'})\n",
    "        \n",
    "        df_300v.reset_index(inplace=True)\n",
    "        df_300v = df_300v.rename(columns = {'index':'Date'})\n",
    "        \n",
    "        df_200v.reset_index(inplace=True)\n",
    "        df_200v = df_200v.rename(columns = {'index':'Date'})\n",
    "        \n",
    "        df_100v.reset_index(inplace=True)\n",
    "        df_100v = df_100v.rename(columns = {'index':'Date'})\n",
    "        \n",
    "        #df_300t['Date'],df_200t['Date'],df_100t['Date'] = df_300t.Date, df_200t.Date, df_100t.Date\n",
    "        #df_300v['Date'],df_200v['Date'],df_100v['Date'] = df_300v.Date, df_200v.Date, df_100v.Date\n",
    "        train300, train200, train100 = TimeSeries.from_dataframe(df_300t, 'Date', 'y'), TimeSeries.from_dataframe(df_200t, 'Date', 'y'), TimeSeries.from_dataframe(df_100t, 'Date', 'y')\n",
    "        val300, val200, val100 = TimeSeries.from_dataframe(df_300v, 'Date', 'y'), TimeSeries.from_dataframe(df_200v, 'Date', 'y'), TimeSeries.from_dataframe(df_100v, 'Date', 'y')\n",
    "\n",
    "        train300_scaled, train200_scaled, train100_scaled = scaler.fit_transform(train300), scaler.fit_transform(train200), scaler.fit_transform(train100)\n",
    "        val300_scaled, val200_scaled, val100_scaled = scaler.transform(val300), scaler.transform(val200), scaler.transform(val100)\n",
    "\n",
    "        pred300, pred200, pred100 = TimeSeries.from_dataframe(pred_300, 'Date', 'y'), TimeSeries.from_dataframe(pred_200, 'Date', 'y'), TimeSeries.from_dataframe(pred_100, 'Date', 'y')\n",
    "        pred300_scaled, pred200_scaled, pred100_scaled = scaler.transform(pred300), scaler.transform(pred200), scaler.transform(pred100)\n",
    "\n",
    "    \n",
    "        MAE300 = mae(val300_scaled,pred300_scaled)\n",
    "        MSE300 = mse(val300_scaled,pred300_scaled)\n",
    "        RMSE300 = rmse(val300_scaled, pred300_scaled)\n",
    "        \n",
    "        MAE200 = mae(val200_scaled,pred200_scaled)\n",
    "        MSE200 = mse(val200_scaled,pred200_scaled)\n",
    "        RMSE200 = rmse(val200_scaled, pred200_scaled)\n",
    "        \n",
    "        MAE100 = mae(val100_scaled,pred100_scaled)\n",
    "        MSE100 = mse(val100_scaled,pred100_scaled)\n",
    "        RMSE100 = rmse(val100_scaled, pred100_scaled)\n",
    "\n",
    "        df_new_row_300 = pd.DataFrame(data=np.array([[name,'Sarima',float(MAE300),float(MSE300),float(RMSE300)]]), columns=['DATA_SET_NAME','Method','MAE','MSE','RMSE'])\n",
    "        df_new_row_200 = pd.DataFrame(data=np.array([[name,'Sarima',float(MAE200),float(MSE200),float(RMSE200)]]), columns=['DATA_SET_NAME','Method','MAE','MSE','RMSE'])\n",
    "        df_new_row_100 = pd.DataFrame(data=np.array([[name,'Sarima',float(MAE100),float(MSE100),float(RMSE100)]]), columns=['DATA_SET_NAME','Method','MAE','MSE','RMSE'])\n",
    "        results_300 = pd.concat([results_300,df_new_row_300], ignore_index=True)\n",
    "        results_200 = pd.concat([results_200,df_new_row_200], ignore_index=True)\n",
    "        results_100 = pd.concat([results_100,df_new_row_100], ignore_index=True)\n",
    "        \n",
    "        print(f\"Processed: {name} {np.array(s_300.prediction).shape}\")\n",
    "        i+=1\n",
    "    except Exception as e:\n",
    "        #print(s.season)\n",
    "        sr = str(e)\n",
    "        print(f\"Error: {name}\\n{sr}\")\n",
    "        print(f\"300: pred={np.array(s_300.prediction.predicted_mean).shape}, {df_300v.shape}\\n200: pred={np.array(s_200.prediction.predicted_mean).shape}, {df_200v.shape}\\n100: pred={np.array(s_100.prediction.predicted_mean).shape}, {df_100v.shape}\")\n",
    "#print(results)\n",
    "\n",
    "print(f\"END {i} {len(fil)}\")\n",
    "results_300.to_csv(r'../../Resultats/Resultats_300/sarima_results_climat.csv', index = False)\n",
    "results_200.to_csv(r'../../Resultats/Resultats_200/sarima_results_climat.csv', index = False)\n",
    "results_100.to_csv(r'../../Resultats/Resultats_100/sarima_results_climat.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
